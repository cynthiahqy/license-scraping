{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FCC Application Scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up selenium webdriver with Firefox\n",
    "* Download Firefox driver [geckodriver](https://github.com/mozilla/geckodriver/releases) for selenium\n",
    "* Move the driver executable into the same directory as this notebook (or update the path to the executable in `scrape_table_for_call_sign`)\n",
    "* `pip install selenium`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import random\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.firefox.options import Options  \n",
    "from selenium.webdriver.support.ui import Select\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions for scraping table for an individual call sign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def element_exists(driver, xpath):\n",
    "    \"\"\"\n",
    "    Helper function that checks if the element at the given xpath exists on\n",
    "    driver's current page. This is used to check if there is a \"Next\" button\n",
    "    -- to scrape through multiple pages of the FCC's paginated tables.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        driver.find_element_by_xpath(xpath)\n",
    "    except NoSuchElementException:\n",
    "        return False\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_callsign(innerHTML):\n",
    "    '''\n",
    "    Helper function that extracts the callsign from Application search results page.\n",
    "    This is used to append the CALLSIGN to each row when scraping table data\n",
    "    '''\n",
    "    soup = BeautifulSoup(innerHTML, \"lxml\")\n",
    "    \n",
    "    # tag attributes for element containing text \"Call Sign = \" \n",
    "    tag_attrs = {\"class\":\"cell-pri-light\", \"valign\":\"bottom\", \"align\":\"left\"}\n",
    "    \n",
    "    # extract callsign as text\n",
    "    callsign = (\n",
    "        soup.find_all(attrs=tag_attrs, limit=1)[0]\n",
    "        .find_all(\"b\")[0]\n",
    "        .get_text()\n",
    "    )\n",
    "    \n",
    "    return callsign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_results(innerHTML):\n",
    "    '''\n",
    "    Helper function that extracts the total number of results across all pages.\n",
    "    This is used to check the scraped table data has the expected number of rows.\n",
    "    '''\n",
    "    \n",
    "    soup = BeautifulSoup(innerHTML, \"lxml\")\n",
    "    \n",
    "    # tag attributes for element containing text \"Matches 1-47 (of 47)\"\n",
    "    tag_attrs = {\"class\":\"cell-pri-dark\", \"width\":\"35%\", \"valign\":\"middle\", \"align\":\"left\"}\n",
    "\n",
    "    # extract results count from inside \"(of 47)\" as integer 47\n",
    "    count = int(soup.find_all(attrs=tag_attrs, limit=1)[0]\n",
    "                .find_all(\"b\")[1]\n",
    "                .get_text()\n",
    "               )\n",
    "    \n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_table_page(innerHTML):\n",
    "    \"\"\"\n",
    "    Extracts the following details from a call sign table page:\n",
    "        * File Number\n",
    "        * Call Sign/Lease ID\n",
    "        * Applicant Name\n",
    "        * FRN\n",
    "        * Purpose - split into Purpose_Main and Purpose_Note\n",
    "        * Radio\n",
    "        * Service\n",
    "        * Receipt\n",
    "        * Date\n",
    "        * Status\n",
    "    Returns as a list of lists.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Extract search data from innerHTML soup\n",
    "    soup = BeautifulSoup(innerHTML, \"lxml\")\n",
    "\n",
    "    CALLSIGN = extract_callsign(innerHTML)\n",
    "    table_page = soup.find(\"table\", {\"summary\": \"Application search results\"})\n",
    "    \n",
    "    # Extract search results as list of rows\n",
    "    rows = table_page.find_all(\"tr\")[1:-1] # note: skip header and pagination rows\n",
    "    \n",
    "    # Extract results by row into table, attaching callsign number\n",
    "    table_data = []\n",
    "    \n",
    "    for row in rows:  \n",
    "        \n",
    "        row_data = [CALLSIGN, ]\n",
    "        cols = row.find_all(\"td\")\n",
    "        \n",
    "        # extract values from each cell in row\n",
    "        for i in range(1, len(cols)): # note: skip the first (index) cell\n",
    "            \n",
    "            # extract cell data, trim leading/trailing whitespace\n",
    "            cell_data = cols[i].get_text().strip()\n",
    "            \n",
    "            # fifth-index cells \"Purpose\" has to get split into two cols\n",
    "            if i == 5:\n",
    "                \n",
    "                purpose = cell_data.split(\"\\n\")\n",
    "                purpose_main = purpose[0].strip()\n",
    "                purpose_note = \"\"\n",
    "                \n",
    "                # sometimes the \"Purpose\" column has an optional note\n",
    "                if len(purpose) >= 2:\n",
    "                    purpose_note = purpose[-1].strip()\n",
    "                    \n",
    "                row_data.append(purpose_main)\n",
    "                row_data.append(purpose_note)\n",
    "            \n",
    "            # other cells can be inserted as normal\n",
    "            else:\n",
    "                row_data.append(cell_data)\n",
    "                \n",
    "        table_data.append(row_data)\n",
    "    \n",
    "    return(table_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cache_to_csv(lst, path):\n",
    "\n",
    "    no_rows = len(lst)\n",
    "    \n",
    "    with open(path, 'w', newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerows(lst)\n",
    "\n",
    "    with open(path, 'r', newline='') as f:\n",
    "        reader = csv.reader(f)\n",
    "        check = list(reader)\n",
    "\n",
    "    assert(len(lst) == len(check))\n",
    "    \n",
    "    print('LOG[INFO]: Wrote %s rows to %s' % (no_rows, path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_html_path(parent_dir, call_sign, page_no):\n",
    "    '''\n",
    "    Helper function that generates path for caching html pages\n",
    "    '''\n",
    "    path = (\"%s/%s-%s.html\" % (parent_dir, call_sign, str(page_no).zfill(3)))\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_tables_for_call_sign(call_sign, html_cache, csv_cache, without_browser=False):\n",
    "    \"\"\"\n",
    "    Scrapes the FCC Application Search database for a given call sign.\n",
    "    \n",
    "    Caches results pages as HTML\n",
    "    \n",
    "    Returns the table as a list of lists, for example:\n",
    "    [\n",
    "      ['File Number', 'Call Sign/Lease ID', 'Applicant Name', 'FRN', 'Purpose_Main', 'Purpose_Note', 'Radio Service', 'Receipt Date', 'Status'],\n",
    "      ['0000214527',  'KNKN555',            'AT&T',           '',    'Amendment',    'Transfer Control', 'AL',        '09/05/2000',   'Granted'],\n",
    "      ...,\n",
    "      ...\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    tick = time.time()\n",
    "    print(\"LOG[INFO]: START %s \" % call_sign)\n",
    "    \n",
    "    \n",
    "    # driver config - start in private\n",
    "    firefox_profile = webdriver.FirefoxProfile()\n",
    "    firefox_profile.set_preference(\"browser.privatebrowsing.autostart\", True)\n",
    "    \n",
    "    # (optional) don't open up browser\n",
    "    firefox_options = Options()\n",
    "    if without_browser:\n",
    "        firefox_options.add_argument(\"--headless\")\n",
    "    \n",
    "    # spin up the webdriver\n",
    "    firefox = webdriver.Firefox(options=firefox_options, executable_path=\"./geckodriver\")\n",
    "    firefox.get(\"https://wireless2.fcc.gov/UlsApp/ApplicationSearch/searchAdvanced.jsp\")\n",
    "    \n",
    "    # specify the call sign that we want to scrape for\n",
    "    callsign = firefox.find_element_by_id(\"ulsCallSign\")\n",
    "    callsign.send_keys(call_sign)\n",
    "    \n",
    "    # set pagination to 100 (maximum)\n",
    "    pagination = Select(firefox.find_element_by_xpath(\"//select[@name='pageSize']\"))\n",
    "    pagination.select_by_visible_text(\"100\")\n",
    "    \n",
    "    # submit the search form, wait for load\n",
    "    # NOTE: find form submission by form element name, not search button image\n",
    "    firefox.find_element_by_name(\"search\").submit()\n",
    "    WebDriverWait(firefox, 45).until(EC.url_changes(\"https://wireless2.fcc.gov/UlsApp/ApplicationSearch/searchAdvanced.jsp\"))\n",
    "    WebDriverWait(firefox, 45).until(lambda _: firefox.execute_script(\"return document.readyState\") == \"complete\")\n",
    "    \n",
    "    # extract and cache html from first page\n",
    "    page = 1\n",
    "    innerHTML = firefox.execute_script(\"return document.body.innerHTML\")\n",
    "    with open(make_html_path(html_cache, call_sign, page), 'w') as f:\n",
    "        f.write(innerHTML)\n",
    "    \n",
    "    # store count of search results for later check\n",
    "    no_results = count_results(innerHTML)\n",
    "    \n",
    "    # Scrape results data from first page\n",
    "    search_data = []\n",
    "    search_data += scrape_table_page(innerHTML)\n",
    "    \n",
    "    # as long there is a link to \"next page\"...\n",
    "    next_page_xpath = \"//*[@title='Next page of results']\"\n",
    "    while element_exists(firefox, next_page_xpath):\n",
    "        # ... go to next page\n",
    "        firefox.find_element_by_xpath(next_page_xpath).click()\n",
    "        # ... wait for load\n",
    "        WebDriverWait(firefox, 45).until(lambda _: firefox.execute_script(\"return document.readyState\") == \"complete\")\n",
    "        # .. increment page counter, extract and cache html \n",
    "        page += 1\n",
    "        innerHTML = firefox.execute_script(\"return document.body.innerHTML\")\n",
    "        with open(make_html_path(html_cache, call_sign, page), 'w') as f:\n",
    "            f.write(innerHTML)\n",
    "        # ... scrape and add page results to all results\n",
    "        search_data += scrape_table_page(innerHTML)\n",
    "\n",
    "    # gracefully close the webdriver\n",
    "    firefox.quit()\n",
    "    \n",
    "    # Check that all results have been scraped\n",
    "    no_rows = len(search_data)\n",
    "    #assert(no_rows == no_results)\n",
    "    print(\"LOG[INFO]: Scraped %s rows out of %s results\" % (no_rows, no_results))\n",
    "\n",
    "    # Cache search results table to csv (all results from all pages)\n",
    "    csv_path = ''.join([csv_cache, call_sign, \".csv\"])\n",
    "    cache_to_csv(search_data, csv_path)\n",
    "    \n",
    "\n",
    "    tock = time.time()\n",
    "    print(\"LOG[INFO]: END %s (%2f seconds)\" % (call_sign, tock - tick))\n",
    "    \n",
    "    return search_data\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrape the tables for a list of call signs (loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in list of all callsigns from file\n",
    "\n",
    "callsign_path = 'data-cache/cma_callsign_lookup.csv'\n",
    "\n",
    "callsigns = pd.read_csv(callsign_path).sort_values(by=['callsign_leaseID'])['callsign_leaseID'].tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_cache = 'search-cache/apps-by-callsign/html/'\n",
    "csv_cache = 'search-cache/apps-by-callsign/csv/'\n",
    "\n",
    "tick = time.time()\n",
    "print(\"LOG[INFO]: START TIME is %s\\n\" % (time.strftime(\"%I:%M %p\")))\n",
    "\n",
    "tries = 0\n",
    "max_tries = 5\n",
    "\n",
    "start_index = 1\n",
    "\n",
    "for cs in callsigns[start_index:2]:    \n",
    "    if tries <= max_tries:\n",
    "        try:\n",
    "            scrape_tables_for_call_sign(cs, html_cache, csv_cache, without_browser=False)\n",
    "            pause = (random.randint(1,30))\n",
    "        except Exception as e:\n",
    "            tries += 1\n",
    "            print(\"Error at %s, located at callsigns[%s]\" % (cs, callsigns.index(cs)))\n",
    "            pause = (random.randint(200,300) * math.sqrt(tries))\n",
    "        finally:\n",
    "            time.sleep(pause)\n",
    "    else:\n",
    "        break\n",
    "\n",
    "tock = time.time()\n",
    "print(\"LOG[INFO]: END\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
